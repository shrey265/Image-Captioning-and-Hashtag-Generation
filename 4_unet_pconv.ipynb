{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# U-Net based Image Segmentation Generator used for mask prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-12T00:24:51.140253Z",
     "iopub.status.busy": "2025-11-12T00:24:51.140027Z",
     "iopub.status.idle": "2025-11-12T00:24:51.206979Z",
     "shell.execute_reply": "2025-11-12T00:24:51.206193Z",
     "shell.execute_reply.started": "2025-11-12T00:24:51.140237Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "\n",
    "# =============================================================================\n",
    "# 1. LIGHTWEIGHT U-NET ARCHITECTURE (SAME AS BEFORE)\n",
    "# =============================================================================\n",
    "\n",
    "class DoubleConv(nn.Module):\n",
    "    \"\"\"Double Convolution block: Conv -> BN -> ReLU -> Conv -> BN -> ReLU\"\"\"\n",
    "    def __init__(self, in_channels, out_channels, mid_channels=None):\n",
    "        super().__init__()\n",
    "        if not mid_channels:\n",
    "            mid_channels = out_channels\n",
    "        \n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(mid_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "\n",
    "\n",
    "class Down(nn.Module):\n",
    "    \"\"\"Downscaling with maxpool then double conv\"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.maxpool_conv = nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            DoubleConv(in_channels, out_channels)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.maxpool_conv(x)\n",
    "\n",
    "\n",
    "class Up(nn.Module):\n",
    "    \"\"\"Upscaling then double conv\"\"\"\n",
    "    def __init__(self, in_channels, out_channels, bilinear=True):\n",
    "        super().__init__()\n",
    "        \n",
    "        if bilinear:\n",
    "            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "            self.conv = DoubleConv(in_channels, out_channels, in_channels // 2)\n",
    "        else:\n",
    "            self.up = nn.ConvTranspose2d(in_channels, in_channels // 2, kernel_size=2, stride=2)\n",
    "            self.conv = DoubleConv(in_channels, out_channels)\n",
    "    \n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.up(x1)\n",
    "        \n",
    "        diffY = x2.size()[2] - x1.size()[2]\n",
    "        diffX = x2.size()[3] - x1.size()[3]\n",
    "        \n",
    "        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n",
    "                        diffY // 2, diffY - diffY // 2])\n",
    "        \n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class LightweightUNet(nn.Module):\n",
    "    \"\"\"Lightweight U-Net for mask prediction\"\"\"\n",
    "    def __init__(self, n_channels=3, n_classes=1, bilinear=True):\n",
    "        super(LightweightUNet, self).__init__()\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.bilinear = bilinear\n",
    "        \n",
    "        self.inc = DoubleConv(n_channels, 32)\n",
    "        self.down1 = Down(32, 64)\n",
    "        self.down2 = Down(64, 128)\n",
    "        self.down3 = Down(128, 256)\n",
    "        factor = 2 if bilinear else 1\n",
    "        self.down4 = Down(256, 512 // factor)\n",
    "        \n",
    "        self.up1 = Up(512, 256 // factor, bilinear)\n",
    "        self.up2 = Up(256, 128 // factor, bilinear)\n",
    "        self.up3 = Up(128, 64 // factor, bilinear)\n",
    "        self.up4 = Up(64, 32, bilinear)\n",
    "        \n",
    "        self.outc = nn.Conv2d(32, n_classes, kernel_size=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x1 = self.inc(x)\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.down3(x3)\n",
    "        x5 = self.down4(x4)\n",
    "        \n",
    "        x = self.up1(x5, x4)\n",
    "        x = self.up2(x, x3)\n",
    "        x = self.up3(x, x2)\n",
    "        x = self.up4(x, x1)\n",
    "        \n",
    "        logits = self.outc(x)\n",
    "        return torch.sigmoid(logits)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 2. SYNTHETIC DISTORTION GENERATOR (SAME AS BEFORE)\n",
    "# =============================================================================\n",
    "\n",
    "class SyntheticDistortionGenerator:\n",
    "    \"\"\"Generate synthetic distortions for training\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def add_gaussian_noise(image, severity=0.1):\n",
    "        \"\"\"Add Gaussian noise\"\"\"\n",
    "        noise = np.random.normal(0, severity * 255, image.shape).astype(np.float32)\n",
    "        noisy = np.clip(image.astype(np.float32) + noise, 0, 255).astype(np.uint8)\n",
    "        \n",
    "        mask = np.zeros(image.shape[:2], dtype=np.uint8)\n",
    "        num_patches = np.random.randint(3, 10)\n",
    "        \n",
    "        for _ in range(num_patches):\n",
    "            h, w = image.shape[:2]\n",
    "            x = np.random.randint(0, max(1, w - 50))\n",
    "            y = np.random.randint(0, max(1, h - 50))\n",
    "            patch_w = np.random.randint(20, min(100, w - x))\n",
    "            patch_h = np.random.randint(20, min(100, h - y))\n",
    "            mask[y:y+patch_h, x:x+patch_w] = 1\n",
    "        \n",
    "        return noisy, mask\n",
    "    \n",
    "    @staticmethod\n",
    "    def add_scratches(image, num_scratches=5):\n",
    "        \"\"\"Add random scratches\"\"\"\n",
    "        scratched = image.copy()\n",
    "        mask = np.zeros(image.shape[:2], dtype=np.uint8)\n",
    "        \n",
    "        h, w = image.shape[:2]\n",
    "        \n",
    "        for _ in range(num_scratches):\n",
    "            x1, y1 = np.random.randint(0, w), np.random.randint(0, h)\n",
    "            x2, y2 = np.random.randint(0, w), np.random.randint(0, h)\n",
    "            thickness = np.random.randint(1, 5)\n",
    "            \n",
    "            cv2.line(scratched, (x1, y1), (x2, y2), (255, 255, 255), thickness)\n",
    "            cv2.line(mask, (x1, y1), (x2, y2), 1, thickness)\n",
    "        \n",
    "        return scratched, mask\n",
    "    \n",
    "    @staticmethod\n",
    "    def add_blur_patches(image, num_patches=3):\n",
    "        \"\"\"Add blurred patches\"\"\"\n",
    "        blurred = image.copy()\n",
    "        mask = np.zeros(image.shape[:2], dtype=np.uint8)\n",
    "        \n",
    "        h, w = image.shape[:2]\n",
    "        \n",
    "        for _ in range(num_patches):\n",
    "            x = np.random.randint(0, max(1, w - 50))\n",
    "            y = np.random.randint(0, max(1, h - 50))\n",
    "            patch_w = np.random.randint(40, min(150, w - x))\n",
    "            patch_h = np.random.randint(40, min(150, h - y))\n",
    "            \n",
    "            patch = blurred[y:y+patch_h, x:x+patch_w]\n",
    "            kernel_size = np.random.choice([15, 21, 31])\n",
    "            blurred_patch = cv2.GaussianBlur(patch, (kernel_size, kernel_size), 0)\n",
    "            blurred[y:y+patch_h, x:x+patch_w] = blurred_patch\n",
    "            \n",
    "            mask[y:y+patch_h, x:x+patch_w] = 1\n",
    "        \n",
    "        return blurred, mask\n",
    "    \n",
    "    @staticmethod\n",
    "    def add_jpeg_artifacts(image, quality=20):\n",
    "        \"\"\"Add JPEG compression artifacts\"\"\"\n",
    "        encode_param = [int(cv2.IMWRITE_JPEG_QUALITY), quality]\n",
    "        _, encimg = cv2.imencode('.jpg', image, encode_param)\n",
    "        compressed = cv2.imdecode(encimg, 1)\n",
    "        \n",
    "        diff = np.abs(image.astype(np.float32) - compressed.astype(np.float32))\n",
    "        diff_gray = cv2.cvtColor(diff.astype(np.uint8), cv2.COLOR_BGR2GRAY)\n",
    "        _, mask = cv2.threshold(diff_gray, 10, 1, cv2.THRESH_BINARY)\n",
    "        \n",
    "        return compressed, mask\n",
    "    \n",
    "    @staticmethod\n",
    "    def add_random_patches(image, num_patches=5):\n",
    "        \"\"\"Add random missing patches\"\"\"\n",
    "        damaged = image.copy()\n",
    "        mask = np.zeros(image.shape[:2], dtype=np.uint8)\n",
    "        \n",
    "        h, w = image.shape[:2]\n",
    "        \n",
    "        for _ in range(num_patches):\n",
    "            x = np.random.randint(0, max(1, w - 30))\n",
    "            y = np.random.randint(0, max(1, h - 30))\n",
    "            patch_w = np.random.randint(20, min(80, w - x))\n",
    "            patch_h = np.random.randint(20, min(80, h - y))\n",
    "            \n",
    "            color = np.random.randint(0, 256, 3).tolist()\n",
    "            damaged[y:y+patch_h, x:x+patch_w] = color\n",
    "            mask[y:y+patch_h, x:x+patch_w] = 1\n",
    "        \n",
    "        return damaged, mask\n",
    "    \n",
    "    @classmethod\n",
    "    def generate_distortion(cls, image):\n",
    "        \"\"\"Apply random distortion and return (distorted_image, mask)\"\"\"\n",
    "        distortion_type = np.random.choice([\n",
    "            'noise', 'scratches', 'blur', 'jpeg', 'patches', 'combined'\n",
    "        ])\n",
    "        \n",
    "        if distortion_type == 'noise':\n",
    "            return cls.add_gaussian_noise(image, severity=np.random.uniform(0.05, 0.15))\n",
    "        elif distortion_type == 'scratches':\n",
    "            return cls.add_scratches(image, num_scratches=np.random.randint(3, 8))\n",
    "        elif distortion_type == 'blur':\n",
    "            return cls.add_blur_patches(image, num_patches=np.random.randint(2, 5))\n",
    "        elif distortion_type == 'jpeg':\n",
    "            return cls.add_jpeg_artifacts(image, quality=np.random.randint(10, 30))\n",
    "        elif distortion_type == 'patches':\n",
    "            return cls.add_random_patches(image, num_patches=np.random.randint(3, 7))\n",
    "        else:  # combined\n",
    "            img, mask = cls.add_scratches(image, num_scratches=2)\n",
    "            img, mask2 = cls.add_blur_patches(img, num_patches=1)\n",
    "            mask = np.logical_or(mask, mask2).astype(np.uint8)\n",
    "            return img, mask\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 3. MODIFIED DATASET CLASS FOR KAGGLE (ACCEPTS FILE LIST)\n",
    "# =============================================================================\n",
    "\n",
    "class DistortionDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for training mask prediction\n",
    "    Modified to work with a list of image files from a single directory\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, image_dir, image_files, transform=None, use_synthetic=True, img_size=256):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            image_dir: Base directory containing all images\n",
    "            image_files: List of image filenames to use (after splitting)\n",
    "            transform: Optional transforms\n",
    "            use_synthetic: Whether to generate synthetic distortions\n",
    "            img_size: Target image size\n",
    "        \"\"\"\n",
    "        self.image_dir = image_dir\n",
    "        self.image_files = image_files  # Only use specified files\n",
    "        self.transform = transform\n",
    "        self.use_synthetic = use_synthetic\n",
    "        self.img_size = img_size\n",
    "        \n",
    "        self.distortion_gen = SyntheticDistortionGenerator()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Load clean image\n",
    "        img_path = os.path.join(self.image_dir, self.image_files[idx])\n",
    "        \n",
    "        try:\n",
    "            image = cv2.imread(img_path)\n",
    "            if image is None:\n",
    "                raise ValueError(f\"Failed to load image: {img_path}\")\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {img_path}: {e}\")\n",
    "            # Return a blank image if loading fails\n",
    "            image = np.zeros((self.img_size, self.img_size, 3), dtype=np.uint8)\n",
    "        \n",
    "        # Resize\n",
    "        image = cv2.resize(image, (self.img_size, self.img_size))\n",
    "        \n",
    "        # Generate synthetic distortion\n",
    "        if self.use_synthetic:\n",
    "            distorted_image, mask = self.distortion_gen.generate_distortion(image)\n",
    "        else:\n",
    "            distorted_image = image\n",
    "            mask = np.zeros(image.shape[:2], dtype=np.uint8)\n",
    "        \n",
    "        # Convert to tensors\n",
    "        distorted_image = torch.from_numpy(distorted_image).permute(2, 0, 1).float() / 255.0\n",
    "        mask = torch.from_numpy(mask).unsqueeze(0).float()\n",
    "        \n",
    "        if self.transform:\n",
    "            state = torch.get_rng_state()\n",
    "            distorted_image = self.transform(distorted_image)\n",
    "            torch.set_rng_state(state)\n",
    "            mask = self.transform(mask)\n",
    "        \n",
    "        return distorted_image, mask\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 4. DATA SPLITTING UTILITY FOR SINGLE DIRECTORY\n",
    "# =============================================================================\n",
    "\n",
    "def split_dataset_from_directory(image_dir, train_ratio=0.7, val_ratio=0.15, test_ratio=0.15, seed=42):\n",
    "    \"\"\"\n",
    "    Split images from a single directory into train/val/test sets\n",
    "    \n",
    "    Args:\n",
    "        image_dir: Directory containing all images\n",
    "        train_ratio: Proportion for training (default: 0.7)\n",
    "        val_ratio: Proportion for validation (default: 0.15)\n",
    "        test_ratio: Proportion for testing (default: 0.15)\n",
    "        seed: Random seed for reproducibility\n",
    "    \n",
    "    Returns:\n",
    "        train_files, val_files, test_files: Lists of filenames for each split\n",
    "    \"\"\"\n",
    "    # Set seed for reproducibility\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Get all image files\n",
    "    all_files = [f for f in os.listdir(image_dir) \n",
    "                 if f.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp'))]\n",
    "    \n",
    "    print(f\"Total images found: {len(all_files)}\")\n",
    "    \n",
    "    # Shuffle\n",
    "    random.shuffle(all_files)\n",
    "    \n",
    "    # Calculate split sizes\n",
    "    total = len(all_files)\n",
    "    train_size = int(total * train_ratio)\n",
    "    val_size = int(total * val_ratio)\n",
    "    \n",
    "    # Split\n",
    "    train_files = all_files[:train_size]\n",
    "    val_files = all_files[train_size:train_size + val_size]\n",
    "    test_files = all_files[train_size + val_size:]\n",
    "    \n",
    "    print(f\"Train: {len(train_files)} images\")\n",
    "    print(f\"Val: {len(val_files)} images\")\n",
    "    print(f\"Test: {len(test_files)} images\")\n",
    "    \n",
    "    return train_files, val_files, test_files\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 5. KAGGLE-SPECIFIC PATH FINDER\n",
    "# =============================================================================\n",
    "\n",
    "def find_flickr8k_path():\n",
    "    \"\"\"\n",
    "    Automatically find Flickr8k dataset path in Kaggle\n",
    "    \"\"\"\n",
    "    possible_paths = [\n",
    "        '/kaggle/input/flickr8k/Images',\n",
    "        '/kaggle/input/flickr-8k/Images',\n",
    "        '/kaggle/input/flickr8k-dataset/Images',\n",
    "        '/kaggle/input/flickr-image-dataset/Images',\n",
    "        '/kaggle/input/flickr8k',\n",
    "        '/kaggle/input/flickr-8k',\n",
    "    ]\n",
    "    \n",
    "    # Try to find the correct path\n",
    "    for path in possible_paths:\n",
    "        if os.path.exists(path):\n",
    "            print(f\"✓ Found Flickr8k at: {path}\")\n",
    "            return path\n",
    "    \n",
    "    # If not found, list available paths\n",
    "    print(\"Could not find Flickr8k automatically. Available input directories:\")\n",
    "    if os.path.exists('/kaggle/input'):\n",
    "        for item in os.listdir('/kaggle/input'):\n",
    "            item_path = os.path.join('/kaggle/input', item)\n",
    "            print(f\"  - {item_path}\")\n",
    "            if os.path.isdir(item_path):\n",
    "                for subitem in os.listdir(item_path)[:5]:  # Show first 5 items\n",
    "                    print(f\"    - {subitem}\")\n",
    "    \n",
    "    raise FileNotFoundError(\"Please manually specify the correct path to Flickr8k images\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 6. LOSS FUNCTIONS (SAME AS BEFORE)\n",
    "# =============================================================================\n",
    "\n",
    "class DiceLoss(nn.Module):\n",
    "    \"\"\"Dice Loss for segmentation\"\"\"\n",
    "    def __init__(self, smooth=1.0):\n",
    "        super(DiceLoss, self).__init__()\n",
    "        self.smooth = smooth\n",
    "    \n",
    "    def forward(self, pred, target):\n",
    "        pred = pred.contiguous().view(-1)\n",
    "        target = target.contiguous().view(-1)\n",
    "        \n",
    "        intersection = (pred * target).sum()\n",
    "        dice = (2. * intersection + self.smooth) / (pred.sum() + target.sum() + self.smooth)\n",
    "        \n",
    "        return 1 - dice\n",
    "\n",
    "\n",
    "class CombinedLoss(nn.Module):\n",
    "    \"\"\"Combination of BCE and Dice Loss\"\"\"\n",
    "    def __init__(self, bce_weight=0.5, dice_weight=0.5):\n",
    "        super(CombinedLoss, self).__init__()\n",
    "        self.bce = nn.BCELoss()\n",
    "        self.dice = DiceLoss()\n",
    "        self.bce_weight = bce_weight\n",
    "        self.dice_weight = dice_weight\n",
    "    \n",
    "    def forward(self, pred, target):\n",
    "        bce_loss = self.bce(pred, target)\n",
    "        dice_loss = self.dice(pred, target)\n",
    "        return self.bce_weight * bce_loss + self.dice_weight * dice_loss\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 7. TRAINING FUNCTIONS (SAME AS BEFORE)\n",
    "# =============================================================================\n",
    "\n",
    "def train_one_epoch(model, dataloader, optimizer, criterion, device):\n",
    "    \"\"\"Train for one epoch\"\"\"\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    pbar = tqdm(dataloader, desc='Training')\n",
    "    for images, masks in pbar:\n",
    "        images = images.to(device)\n",
    "        masks = masks.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, masks)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        pbar.set_postfix({'loss': loss.item()})\n",
    "    \n",
    "    return running_loss / len(dataloader)\n",
    "\n",
    "\n",
    "def validate(model, dataloader, criterion, device):\n",
    "    \"\"\"Validate the model\"\"\"\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, masks in tqdm(dataloader, desc='Validation'):\n",
    "            images = images.to(device)\n",
    "            masks = masks.to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, masks)\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "    \n",
    "    return running_loss / len(dataloader)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 8. MODIFIED TRAINING FUNCTION FOR KAGGLE (SINGLE DIRECTORY)\n",
    "# =============================================================================\n",
    "\n",
    "def train_mask_predictor_kaggle(\n",
    "    image_dir=None,\n",
    "    train_ratio=0.7,\n",
    "    val_ratio=0.15,\n",
    "    test_ratio=0.15,\n",
    "    epochs=50,\n",
    "    batch_size=8,\n",
    "    learning_rate=0.001,\n",
    "    img_size=256,\n",
    "    save_dir='/kaggle/working/checkpoints',\n",
    "    device=None,\n",
    "    num_workers=2,\n",
    "    seed=42\n",
    "):\n",
    "    \"\"\"\n",
    "    Modified training function for Kaggle with single directory\n",
    "    \n",
    "    Args:\n",
    "        image_dir: Directory containing all images (if None, auto-detect Flickr8k)\n",
    "        train_ratio: Proportion for training\n",
    "        val_ratio: Proportion for validation\n",
    "        test_ratio: Proportion for testing\n",
    "        epochs: Number of training epochs\n",
    "        batch_size: Batch size\n",
    "        learning_rate: Learning rate\n",
    "        img_size: Image size (square)\n",
    "        save_dir: Directory to save checkpoints (default: /kaggle/working/)\n",
    "        device: Device to train on (auto-detect if None)\n",
    "        num_workers: Number of data loading workers\n",
    "        seed: Random seed\n",
    "    \"\"\"\n",
    "    \n",
    "    # Auto-detect device\n",
    "    if device is None:\n",
    "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Auto-detect Flickr8k path if not provided\n",
    "    if image_dir is None:\n",
    "        image_dir = find_flickr8k_path()\n",
    "    \n",
    "    print(f\"Loading images from: {image_dir}\")\n",
    "    \n",
    "    # Create save directory\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    print(f\"Checkpoints will be saved to: {save_dir}\")\n",
    "    \n",
    "    # Split dataset\n",
    "    print(\"\\nSplitting dataset...\")\n",
    "    train_files, val_files, test_files = split_dataset_from_directory(\n",
    "        image_dir, train_ratio, val_ratio, test_ratio, seed\n",
    "    )\n",
    "    \n",
    "    # Save split information\n",
    "    split_info = {\n",
    "        'train': train_files,\n",
    "        'val': val_files,\n",
    "        'test': test_files\n",
    "    }\n",
    "    import pickle\n",
    "    with open(os.path.join(save_dir, 'data_split.pkl'), 'wb') as f:\n",
    "        pickle.dump(split_info, f)\n",
    "    print(f\"Data split saved to: {os.path.join(save_dir, 'data_split.pkl')}\")\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = DistortionDataset(image_dir, train_files, img_size=img_size)\n",
    "    val_dataset = DistortionDataset(image_dir, val_files, img_size=img_size)\n",
    "    \n",
    "    # Create dataloaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=True, \n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True if device == 'cuda' else False\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=False, \n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True if device == 'cuda' else False\n",
    "    )\n",
    "    \n",
    "    # Create model\n",
    "    print(\"\\nInitializing model...\")\n",
    "    model = LightweightUNet(n_channels=3, n_classes=1, bilinear=True)\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Count parameters\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"Total parameters: {total_params:,}\")\n",
    "    \n",
    "    # Loss and optimizer\n",
    "    criterion = CombinedLoss(bce_weight=0.5, dice_weight=0.5)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.5, patience=5, verbose=True\n",
    "    )\n",
    "    \n",
    "    # Training loop\n",
    "    best_val_loss = float('inf')\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    print(\"\\nStarting training...\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        print(f'\\nEpoch {epoch+1}/{epochs}')\n",
    "        print(\"-\" * 70)\n",
    "        \n",
    "        # Train\n",
    "        train_loss = train_one_epoch(model, train_loader, optimizer, criterion, device)\n",
    "        train_losses.append(train_loss)\n",
    "        print(f'Training Loss: {train_loss:.4f}')\n",
    "        \n",
    "        # Validate\n",
    "        val_loss = validate(model, val_loader, criterion, device)\n",
    "        val_losses.append(val_loss)\n",
    "        print(f'Validation Loss: {val_loss:.4f}')\n",
    "        \n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        # Save best model\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            checkpoint_path = os.path.join(save_dir, 'best_model.pth')\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'train_loss': train_loss,\n",
    "                'val_loss': val_loss,\n",
    "                'best_val_loss': best_val_loss,\n",
    "            }, checkpoint_path)\n",
    "            print(f'✓ Saved best model (val_loss: {val_loss:.4f}) to {checkpoint_path}')\n",
    "        \n",
    "        # Save checkpoint every 10 epochs\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            checkpoint_path = os.path.join(save_dir, f'checkpoint_epoch_{epoch+1}.pth')\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'train_loss': train_loss,\n",
    "                'val_loss': val_loss,\n",
    "            }, checkpoint_path)\n",
    "            print(f'✓ Saved checkpoint to {checkpoint_path}')\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"Training completed!\")\n",
    "    print(f\"Best validation loss: {best_val_loss:.4f}\")\n",
    "    print(f\"Models saved in: {save_dir}\")\n",
    "    \n",
    "    # Save training history\n",
    "    history = {\n",
    "        'train_losses': train_losses,\n",
    "        'val_losses': val_losses,\n",
    "        'best_val_loss': best_val_loss\n",
    "    }\n",
    "    import pickle\n",
    "    with open(os.path.join(save_dir, 'training_history.pkl'), 'wb') as f:\n",
    "        pickle.dump(history, f)\n",
    "    \n",
    "    return model, history\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 9. INFERENCE CLASS (SAME AS BEFORE)\n",
    "# =============================================================================\n",
    "\n",
    "class MaskPredictor:\n",
    "    \"\"\"Inference wrapper for mask prediction\"\"\"\n",
    "    \n",
    "    def __init__(self, checkpoint_path, device=None):\n",
    "        if device is None:\n",
    "            device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        \n",
    "        self.device = device\n",
    "        self.model = LightweightUNet(n_channels=3, n_classes=1, bilinear=True)\n",
    "        \n",
    "        # Load checkpoint\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "        self.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        self.model.to(device)\n",
    "        self.model.eval()\n",
    "        \n",
    "        print(f\"Model loaded from {checkpoint_path}\")\n",
    "        print(f\"Trained for {checkpoint.get('epoch', 'unknown')} epochs\")\n",
    "        print(f\"Best val loss: {checkpoint.get('best_val_loss', checkpoint.get('val_loss', 'unknown'))}\")\n",
    "    \n",
    "    def predict(self, image, threshold=0.5):\n",
    "        \"\"\"\n",
    "        Predict mask for a single image\n",
    "        \n",
    "        Args:\n",
    "            image: numpy array (H, W, 3) in RGB format, values [0, 255]\n",
    "            threshold: Threshold for binary mask\n",
    "        \n",
    "        Returns:\n",
    "            mask: numpy array (H, W) with values [0, 1]\n",
    "        \"\"\"\n",
    "        original_size = image.shape[:2]\n",
    "        \n",
    "        # Preprocess\n",
    "        image_tensor = torch.from_numpy(image).permute(2, 0, 1).unsqueeze(0).float() / 255.0\n",
    "        image_tensor = image_tensor.to(self.device)\n",
    "        \n",
    "        # Predict\n",
    "        with torch.no_grad():\n",
    "            output = self.model(image_tensor)\n",
    "        \n",
    "        # Postprocess\n",
    "        mask = output.squeeze().cpu().numpy()\n",
    "        mask = (mask > threshold).astype(np.uint8)\n",
    "        \n",
    "        # Resize to original size if needed\n",
    "        if mask.shape != original_size:\n",
    "            mask = cv2.resize(mask, (original_size[1], original_size[0]), \n",
    "                            interpolation=cv2.INTER_NEAREST)\n",
    "        \n",
    "        return 1 - mask\n",
    "    \n",
    "    def predict_batch(self, images, threshold=0.5):\n",
    "        \"\"\"Predict masks for a batch of images\"\"\"\n",
    "        masks = []\n",
    "        for image in images:\n",
    "            mask = self.predict(image, threshold)\n",
    "            masks.append(mask)\n",
    "        return masks\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 10. VISUALIZATION UTILITIES\n",
    "# =============================================================================\n",
    "\n",
    "def visualize_predictions(images_list, save_path='/kaggle/working/predictions.png'):\n",
    "    \"\"\"\n",
    "    Visualize multiple predictions in a grid\n",
    "    \n",
    "    Args:\n",
    "        images_list: List of tuples (image, ground_truth_mask, predicted_mask)\n",
    "        save_path: Path to save visualization\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    n_samples = len(images_list)\n",
    "    fig, axes = plt.subplots(n_samples, 3, figsize=(15, 5 * n_samples))\n",
    "    \n",
    "    if n_samples == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    \n",
    "    for idx, (image, gt_mask, pred_mask) in enumerate(images_list):\n",
    "        axes[idx, 0].imshow(image)\n",
    "        axes[idx, 0].set_title('Original Image')\n",
    "        axes[idx, 0].axis('off')\n",
    "        \n",
    "        axes[idx, 1].imshow(gt_mask, cmap='gray')\n",
    "        axes[idx, 1].set_title('Ground Truth Mask')\n",
    "        axes[idx, 1].axis('off')\n",
    "        \n",
    "        axes[idx, 2].imshow(pred_mask, cmap='gray')\n",
    "        axes[idx, 2].set_title('Predicted Mask')\n",
    "        axes[idx, 2].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "    print(f\"Visualization saved to {save_path}\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_training_history(history, save_path='/kaggle/working/training_history.png'):\n",
    "    \"\"\"Plot training and validation loss\"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(history['train_losses'], label='Training Loss', linewidth=2)\n",
    "    plt.plot(history['val_losses'], label='Validation Loss', linewidth=2)\n",
    "    plt.xlabel('Epoch', fontsize=12)\n",
    "    plt.ylabel('Loss', fontsize=12)\n",
    "    plt.title('Training History', fontsize=14)\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "    print(f\"Training history plot saved to {save_path}\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 11. COMPLETE KAGGLE USAGE EXAMPLE\n",
    "# =============================================================================\n",
    "\n",
    "def kaggle_complete_pipeline():\n",
    "    \"\"\"\n",
    "    Complete pipeline for Kaggle notebook\n",
    "    Run this in a Kaggle notebook cell\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(\"LIGHTWEIGHT U-NET MASK PREDICTOR - KAGGLE PIPELINE\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Step 1: Train the model\n",
    "    print(\"\\n[STEP 1] Training model...\")\n",
    "    model, history = train_mask_predictor_kaggle(\n",
    "        image_dir=None,  # Auto-detect Flickr8k\n",
    "        train_ratio=0.7,\n",
    "        val_ratio=0.15,\n",
    "        test_ratio=0.15,\n",
    "        epochs=30,  # Start with 30 epochs for faster testing\n",
    "        batch_size=16,  # Increase if you have enough GPU memory\n",
    "        learning_rate=0.001,\n",
    "        img_size=256,\n",
    "        save_dir='/kaggle/working/checkpoints',\n",
    "        num_workers=2,\n",
    "        seed=42\n",
    "    )\n",
    "    \n",
    "    # Step 2: Plot training history\n",
    "    print(\"\\n[STEP 2] Plotting training history...\")\n",
    "    plot_training_history(history)\n",
    "    \n",
    "    # Step 3: Test on some images\n",
    "    print(\"\\n[STEP 3] Testing on sample images...\")\n",
    "    predictor = MaskPredictor('/kaggle/working/checkpoints/best_model.pth')\n",
    "    \n",
    "    # Load test data split\n",
    "    import pickle\n",
    "    with open('/kaggle/working/checkpoints/data_split.pkl', 'rb') as f:\n",
    "        split_info = pickle.load(f)\n",
    "    \n",
    "    # Test on a few images\n",
    "    image_dir = find_flickr8k_path()\n",
    "    test_files = split_info['test'][:5]  # First 5 test images\n",
    "    \n",
    "    visualizations = []\n",
    "    for img_file in test_files:\n",
    "        img_path = os.path.join(image_dir, img_file)\n",
    "        image = cv2.imread(img_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Generate synthetic distortion for testing\n",
    "        distortion_gen = SyntheticDistortionGenerator()\n",
    "        distorted, gt_mask = distortion_gen.generate_distortion(image)\n",
    "        \n",
    "        # Predict mask\n",
    "        pred_mask = predictor.predict(distorted)\n",
    "        \n",
    "        visualizations.append((distorted, gt_mask, pred_mask))\n",
    "    \n",
    "    # Visualize results\n",
    "    visualize_predictions(visualizations)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"Pipeline completed successfully!\")\n",
    "    print(\"=\" * 70)\n",
    "    print(\"\\nSaved files:\")\n",
    "    print(f\"  - Best model: /kaggle/working/checkpoints/best_model.pth\")\n",
    "    print(f\"  - Training history: /kaggle/working/checkpoints/training_history.pkl\")\n",
    "    print(f\"  - Data split: /kaggle/working/checkpoints/data_split.pkl\")\n",
    "    print(f\"  - Visualizations: /kaggle/working/predictions.png\")\n",
    "    print(f\"  - Training plot: /kaggle/working/training_history.png\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 12. QUICK TEST FUNCTION\n",
    "# =============================================================================\n",
    "\n",
    "def quick_test():\n",
    "    \"\"\"Quick test to verify everything works\"\"\"\n",
    "    print(\"Running quick test...\")\n",
    "    \n",
    "    # Check GPU\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    print(f\"Device: {device}\")\n",
    "    \n",
    "    # Check path\n",
    "    try:\n",
    "        image_dir = find_flickr8k_path()\n",
    "        files = os.listdir(image_dir)[:5]\n",
    "        print(f\"Found {len(files)} sample files: {files}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return\n",
    "    \n",
    "    # Create a small model\n",
    "    model = LightweightUNet()\n",
    "    dummy_input = torch.randn(1, 3, 256, 256)\n",
    "    output = model(dummy_input)\n",
    "    print(f\"Model output shape: {output.shape}\")\n",
    "    \n",
    "    print(\"✓ Quick test passed!\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# KAGGLE NOTEBOOK EXECUTION\n",
    "# =============================================================================\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Uncomment one of these based on what you want to do:\n",
    "    \n",
    "    # Option 1: Quick test (recommended first)\n",
    "    # quick_test()\n",
    "    \n",
    "    # Option 2: Run complete pipeline\n",
    "    # kaggle_complete_pipeline()\n",
    "    \n",
    "    # Option 3: Train only with custom parameters\n",
    "    # model, history = train_mask_predictor_kaggle(\n",
    "    #     epochs=50,\n",
    "    #     batch_size=16,\n",
    "    #     img_size=256\n",
    "    # )\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Partial Convultion based Inpainting Generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-12T00:24:51.208344Z",
     "iopub.status.busy": "2025-11-12T00:24:51.208104Z",
     "iopub.status.idle": "2025-11-12T00:24:53.260945Z",
     "shell.execute_reply": "2025-11-12T00:24:53.260388Z",
     "shell.execute_reply.started": "2025-11-12T00:24:51.208316Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# =============================================================================\n",
    "# 1. PARTIAL CONVOLUTION LAYER (Core Innovation)\n",
    "# =============================================================================\n",
    "\n",
    "class PartialConv2d(nn.Module):\n",
    "    \"\"\"\n",
    "    Partial Convolution Layer\n",
    "    \n",
    "    Key innovation: Convolution that handles irregular masks\n",
    "    - Only convolves over valid (unmasked) pixels\n",
    "    - Automatically updates mask as it goes deeper\n",
    "    \n",
    "    Paper: \"Image Inpainting for Irregular Holes Using Partial Convolutions\"\n",
    "           Liu et al., ECCV 2018\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1,\n",
    "                 padding=0, dilation=1, groups=1, bias=True):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_conv = nn.Conv2d(\n",
    "            in_channels, out_channels, kernel_size,\n",
    "            stride, padding, dilation, groups, bias\n",
    "        )\n",
    "        \n",
    "        self.mask_conv = nn.Conv2d(\n",
    "            in_channels, out_channels, kernel_size,\n",
    "            stride, padding, dilation, groups, False\n",
    "        )\n",
    "        \n",
    "        self.input_conv.apply(self.weights_init('kaiming'))\n",
    "        \n",
    "        # Mask convolution: fixed weights (all ones)\n",
    "        torch.nn.init.constant_(self.mask_conv.weight, 1.0)\n",
    "        \n",
    "        # Freeze mask convolution weights\n",
    "        for param in self.mask_conv.parameters():\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    def weights_init(self, init_type='kaiming'):\n",
    "        def init_fun(m):\n",
    "            classname = m.__class__.__name__\n",
    "            if (classname.find('Conv') == 0 or classname.find('Linear') == 0) and hasattr(m, 'weight'):\n",
    "                if init_type == 'kaiming':\n",
    "                    nn.init.kaiming_normal_(m.weight.data, a=0, mode='fan_in')\n",
    "                elif init_type == 'xavier':\n",
    "                    nn.init.xavier_normal_(m.weight.data, gain=1.0)\n",
    "                else:\n",
    "                    raise NotImplementedError(f'Initialization {init_type} not supported')\n",
    "                if hasattr(m, 'bias') and m.bias is not None:\n",
    "                    nn.init.constant_(m.bias.data, 0.0)\n",
    "        return init_fun\n",
    "    \n",
    "    def forward(self, input_x, mask):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_x: [B, C, H, W] - Image with holes\n",
    "            mask: [B, C, H, W] - Binary mask (1=valid, 0=hole)\n",
    "        \n",
    "        Returns:\n",
    "            output: [B, C', H', W'] - Convolved output\n",
    "            updated_mask: [B, C', H', W'] - Updated mask\n",
    "        \"\"\"\n",
    "        # Apply convolution\n",
    "        with torch.no_grad():\n",
    "            # Sum of mask values in each kernel window\n",
    "            mask_sum = self.mask_conv(mask)\n",
    "        \n",
    "        # Avoid division by zero\n",
    "        mask_sum = mask_sum.clamp(min=1e-8)\n",
    "        \n",
    "        # Apply convolution and normalize by mask sum\n",
    "        # This ensures we only use valid pixels\n",
    "        output = self.input_conv(input_x * mask)\n",
    "        \n",
    "        # Kernel size for normalization\n",
    "        kernel_size = self.input_conv.kernel_size[0] * self.input_conv.kernel_size[1]\n",
    "        output = output * (kernel_size / mask_sum)\n",
    "        \n",
    "        # Update mask: 1 if any valid pixel in receptive field, else 0\n",
    "        updated_mask = torch.ones_like(mask_sum)\n",
    "        updated_mask[mask_sum == 0] = 0\n",
    "        \n",
    "        return output, updated_mask\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 2. PARTIAL CONVOLUTION U-NET ARCHITECTURE\n",
    "# =============================================================================\n",
    "\n",
    "class PartialConvUNet(nn.Module):\n",
    "    \"\"\"\n",
    "    U-Net with Partial Convolutions for Image Inpainting\n",
    "    \n",
    "    Architecture:\n",
    "    - Encoder: 8 layers with partial convolutions\n",
    "    - Decoder: 8 layers with partial convolutions + skip connections\n",
    "    - Handles irregular masks naturally\n",
    "    \"\"\"\n",
    "    def __init__(self, input_channels=3, output_channels=3):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Encoder layers\n",
    "        self.enc1 = PartialConv2d(input_channels, 64, 7, stride=2, padding=3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        self.enc2 = PartialConv2d(64, 128, 5, stride=2, padding=2, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(128)\n",
    "        \n",
    "        self.enc3 = PartialConv2d(128, 256, 5, stride=2, padding=2, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(256)\n",
    "        \n",
    "        self.enc4 = PartialConv2d(256, 512, 3, stride=2, padding=1, bias=False)\n",
    "        self.bn4 = nn.BatchNorm2d(512)\n",
    "        \n",
    "        self.enc5 = PartialConv2d(512, 512, 3, stride=2, padding=1, bias=False)\n",
    "        self.bn5 = nn.BatchNorm2d(512)\n",
    "        \n",
    "        self.enc6 = PartialConv2d(512, 512, 3, stride=2, padding=1, bias=False)\n",
    "        self.bn6 = nn.BatchNorm2d(512)\n",
    "        \n",
    "        self.enc7 = PartialConv2d(512, 512, 3, stride=2, padding=1, bias=False)\n",
    "        self.bn7 = nn.BatchNorm2d(512)\n",
    "        \n",
    "        self.enc8 = PartialConv2d(512, 512, 3, stride=2, padding=1, bias=False)\n",
    "        self.bn8 = nn.BatchNorm2d(512)\n",
    "        \n",
    "        # Decoder layers\n",
    "        self.dec8 = PartialConv2d(512, 512, 3, stride=1, padding=1, bias=False)\n",
    "        self.dbn8 = nn.BatchNorm2d(512)\n",
    "        \n",
    "        self.dec7 = PartialConv2d(512 + 512, 512, 3, stride=1, padding=1, bias=False)\n",
    "        self.dbn7 = nn.BatchNorm2d(512)\n",
    "        \n",
    "        self.dec6 = PartialConv2d(512 + 512, 512, 3, stride=1, padding=1, bias=False)\n",
    "        self.dbn6 = nn.BatchNorm2d(512)\n",
    "        \n",
    "        self.dec5 = PartialConv2d(512 + 512, 512, 3, stride=1, padding=1, bias=False)\n",
    "        self.dbn5 = nn.BatchNorm2d(512)\n",
    "        \n",
    "        self.dec4 = PartialConv2d(512 + 512, 256, 3, stride=1, padding=1, bias=False)\n",
    "        self.dbn4 = nn.BatchNorm2d(256)\n",
    "        \n",
    "        self.dec3 = PartialConv2d(256 + 256, 128, 3, stride=1, padding=1, bias=False)\n",
    "        self.dbn3 = nn.BatchNorm2d(128)\n",
    "        \n",
    "        self.dec2 = PartialConv2d(128 + 128, 64, 3, stride=1, padding=1, bias=False)\n",
    "        self.dbn2 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        self.dec1 = PartialConv2d(64 + 64, output_channels, 3, stride=1, padding=1, bias=True)\n",
    "        \n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.leaky_relu = nn.LeakyReLU(0.2, inplace=True)\n",
    "    \n",
    "    def forward(self, input_img, mask):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_img: [B, 3, H, W] - Image with holes (holes filled with mean)\n",
    "            mask: [B, 1, H, W] - Binary mask (1=valid, 0=hole)\n",
    "        \n",
    "        Returns:\n",
    "            output: [B, 3, H, W] - Inpainted image\n",
    "        \"\"\"\n",
    "        # Expand mask to match input channels\n",
    "        mask = mask.repeat(1, input_img.shape[1], 1, 1)\n",
    "        \n",
    "        # Encoder with partial convolutions\n",
    "        e1, m1 = self.enc1(input_img, mask)\n",
    "        e1 = self.bn1(e1)\n",
    "        e1 = self.relu(e1)\n",
    "        \n",
    "        e2, m2 = self.enc2(e1, m1)\n",
    "        e2 = self.bn2(e2)\n",
    "        e2 = self.relu(e2)\n",
    "        \n",
    "        e3, m3 = self.enc3(e2, m2)\n",
    "        e3 = self.bn3(e3)\n",
    "        e3 = self.relu(e3)\n",
    "        \n",
    "        e4, m4 = self.enc4(e3, m3)\n",
    "        e4 = self.bn4(e4)\n",
    "        e4 = self.relu(e4)\n",
    "        \n",
    "        e5, m5 = self.enc5(e4, m4)\n",
    "        e5 = self.bn5(e5)\n",
    "        e5 = self.relu(e5)\n",
    "        \n",
    "        e6, m6 = self.enc6(e5, m5)\n",
    "        e6 = self.bn6(e6)\n",
    "        e6 = self.relu(e6)\n",
    "        \n",
    "        e7, m7 = self.enc7(e6, m6)\n",
    "        e7 = self.bn7(e7)\n",
    "        e7 = self.relu(e7)\n",
    "        \n",
    "        e8, m8 = self.enc8(e7, m7)\n",
    "        e8 = self.bn8(e8)\n",
    "        e8 = self.relu(e8)\n",
    "        \n",
    "        # Decoder with skip connections\n",
    "        d8, dm8 = self.dec8(e8, m8)\n",
    "        d8 = self.dbn8(d8)\n",
    "        d8 = self.leaky_relu(d8)\n",
    "        d8 = F.interpolate(d8, scale_factor=2, mode='nearest')\n",
    "        dm8 = F.interpolate(dm8, scale_factor=2, mode='nearest')\n",
    "        \n",
    "        d7 = torch.cat([d8, e7], dim=1)\n",
    "        dm7 = torch.cat([dm8, m7], dim=1)\n",
    "        d7, dm7 = self.dec7(d7, dm7)\n",
    "        d7 = self.dbn7(d7)\n",
    "        d7 = self.leaky_relu(d7)\n",
    "        d7 = F.interpolate(d7, scale_factor=2, mode='nearest')\n",
    "        dm7 = F.interpolate(dm7, scale_factor=2, mode='nearest')\n",
    "        \n",
    "        d6 = torch.cat([d7, e6], dim=1)\n",
    "        dm6 = torch.cat([dm7, m6], dim=1)\n",
    "        d6, dm6 = self.dec6(d6, dm6)\n",
    "        d6 = self.dbn6(d6)\n",
    "        d6 = self.leaky_relu(d6)\n",
    "        d6 = F.interpolate(d6, scale_factor=2, mode='nearest')\n",
    "        dm6 = F.interpolate(dm6, scale_factor=2, mode='nearest')\n",
    "        \n",
    "        d5 = torch.cat([d6, e5], dim=1)\n",
    "        dm5 = torch.cat([dm6, m5], dim=1)\n",
    "        d5, dm5 = self.dec5(d5, dm5)\n",
    "        d5 = self.dbn5(d5)\n",
    "        d5 = self.leaky_relu(d5)\n",
    "        d5 = F.interpolate(d5, scale_factor=2, mode='nearest')\n",
    "        dm5 = F.interpolate(dm5, scale_factor=2, mode='nearest')\n",
    "        \n",
    "        d4 = torch.cat([d5, e4], dim=1)\n",
    "        dm4 = torch.cat([dm5, m4], dim=1)\n",
    "        d4, dm4 = self.dec4(d4, dm4)\n",
    "        d4 = self.dbn4(d4)\n",
    "        d4 = self.leaky_relu(d4)\n",
    "        d4 = F.interpolate(d4, scale_factor=2, mode='nearest')\n",
    "        dm4 = F.interpolate(dm4, scale_factor=2, mode='nearest')\n",
    "        \n",
    "        d3 = torch.cat([d4, e3], dim=1)\n",
    "        dm3 = torch.cat([dm4, m3], dim=1)\n",
    "        d3, dm3 = self.dec3(d3, dm3)\n",
    "        d3 = self.dbn3(d3)\n",
    "        d3 = self.leaky_relu(d3)\n",
    "        d3 = F.interpolate(d3, scale_factor=2, mode='nearest')\n",
    "        dm3 = F.interpolate(dm3, scale_factor=2, mode='nearest')\n",
    "        \n",
    "        d2 = torch.cat([d3, e2], dim=1)\n",
    "        dm2 = torch.cat([dm3, m2], dim=1)\n",
    "        d2, dm2 = self.dec2(d2, dm2)\n",
    "        d2 = self.dbn2(d2)\n",
    "        d2 = self.leaky_relu(d2)\n",
    "        d2 = F.interpolate(d2, scale_factor=2, mode='nearest')\n",
    "        dm2 = F.interpolate(dm2, scale_factor=2, mode='nearest')\n",
    "        \n",
    "        d1 = torch.cat([d2, e1], dim=1)\n",
    "        dm1 = torch.cat([dm2, m1], dim=1)\n",
    "        d1, _ = self.dec1(d1, dm1)\n",
    "        d1 = F.interpolate(d1, scale_factor=2, mode='nearest')\n",
    "        \n",
    "        # Tanh to get output in [-1, 1] range\n",
    "        output = torch.tanh(d1)\n",
    "        \n",
    "        return output\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 3. LOSS FUNCTIONS\n",
    "# =============================================================================\n",
    "\n",
    "class VGGPerceptualLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Perceptual Loss using VGG16\n",
    "    Measures feature-level similarity\n",
    "    \"\"\"\n",
    "    def __init__(self, device='cuda'):\n",
    "        super().__init__()\n",
    "        \n",
    "        vgg16 = models.vgg16(pretrained=True).features.to(device).eval()\n",
    "        \n",
    "        # Extract specific layers\n",
    "        self.slice1 = nn.Sequential(*list(vgg16.children())[:4])   # relu1_2\n",
    "        self.slice2 = nn.Sequential(*list(vgg16.children())[4:9])  # relu2_2\n",
    "        self.slice3 = nn.Sequential(*list(vgg16.children())[9:16]) # relu3_3\n",
    "        \n",
    "        # Freeze weights\n",
    "        for param in self.parameters():\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    def forward(self, pred, target):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            pred: [B, 3, H, W] - Predicted image\n",
    "            target: [B, 3, H, W] - Ground truth image\n",
    "        \"\"\"\n",
    "        # Normalize to ImageNet stats\n",
    "        mean = torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1).to(pred.device)\n",
    "        std = torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1).to(pred.device)\n",
    "        \n",
    "        pred = (pred - mean) / std\n",
    "        target = (target - mean) / std\n",
    "        \n",
    "        # Extract features\n",
    "        pred_f1 = self.slice1(pred)\n",
    "        pred_f2 = self.slice2(pred_f1)\n",
    "        pred_f3 = self.slice3(pred_f2)\n",
    "        \n",
    "        target_f1 = self.slice1(target)\n",
    "        target_f2 = self.slice2(target_f1)\n",
    "        target_f3 = self.slice3(target_f2)\n",
    "        \n",
    "        # Compute L1 loss at each layer\n",
    "        loss = (\n",
    "            F.l1_loss(pred_f1, target_f1) +\n",
    "            F.l1_loss(pred_f2, target_f2) +\n",
    "            F.l1_loss(pred_f3, target_f3)\n",
    "        )\n",
    "        \n",
    "        return loss\n",
    "\n",
    "\n",
    "class StyleLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Style Loss using Gram matrices\n",
    "    Ensures texture consistency\n",
    "    \"\"\"\n",
    "    def __init__(self, device='cuda'):\n",
    "        super().__init__()\n",
    "        \n",
    "        vgg16 = models.vgg16(pretrained=True).features.to(device).eval()\n",
    "        self.slice1 = nn.Sequential(*list(vgg16.children())[:4])\n",
    "        self.slice2 = nn.Sequential(*list(vgg16.children())[4:9])\n",
    "        self.slice3 = nn.Sequential(*list(vgg16.children())[9:16])\n",
    "        \n",
    "        for param in self.parameters():\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    def gram_matrix(self, x):\n",
    "        \"\"\"Compute Gram matrix\"\"\"\n",
    "        B, C, H, W = x.shape\n",
    "        features = x.view(B, C, H * W)\n",
    "        gram = torch.bmm(features, features.transpose(1, 2))\n",
    "        gram = gram / (C * H * W)\n",
    "        return gram\n",
    "    \n",
    "    def forward(self, pred, target):\n",
    "        # Normalize\n",
    "        mean = torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1).to(pred.device)\n",
    "        std = torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1).to(pred.device)\n",
    "        \n",
    "        pred = (pred - mean) / std\n",
    "        target = (target - mean) / std\n",
    "        \n",
    "        # Extract features\n",
    "        pred_f1 = self.slice1(pred)\n",
    "        pred_f2 = self.slice2(pred_f1)\n",
    "        pred_f3 = self.slice3(pred_f2)\n",
    "        \n",
    "        target_f1 = self.slice1(target)\n",
    "        target_f2 = self.slice2(target_f1)\n",
    "        target_f3 = self.slice3(target_f2)\n",
    "        \n",
    "        # Compute Gram matrices and loss\n",
    "        loss = (\n",
    "            F.l1_loss(self.gram_matrix(pred_f1), self.gram_matrix(target_f1)) +\n",
    "            F.l1_loss(self.gram_matrix(pred_f2), self.gram_matrix(target_f2)) +\n",
    "            F.l1_loss(self.gram_matrix(pred_f3), self.gram_matrix(target_f3))\n",
    "        )\n",
    "        \n",
    "        return loss\n",
    "\n",
    "\n",
    "class TotalVariationLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Total Variation Loss\n",
    "    Encourages spatial smoothness\n",
    "    \"\"\"\n",
    "    def forward(self, x):\n",
    "        batch_size, c, h, w = x.shape\n",
    "        \n",
    "        # Horizontal differences\n",
    "        tv_h = torch.abs(x[:, :, 1:, :] - x[:, :, :-1, :]).sum()\n",
    "        \n",
    "        # Vertical differences\n",
    "        tv_w = torch.abs(x[:, :, :, 1:] - x[:, :, :, :-1]).sum()\n",
    "        \n",
    "        return (tv_h + tv_w) / (batch_size * c * h * w)\n",
    "\n",
    "\n",
    "class InpaintingLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Combined loss for inpainting\n",
    "    \"\"\"\n",
    "    def __init__(self, device='cuda'):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.l1_loss = nn.L1Loss()\n",
    "        self.perceptual_loss = VGGPerceptualLoss(device)\n",
    "        self.style_loss = StyleLoss(device)\n",
    "        self.tv_loss = TotalVariationLoss()\n",
    "    \n",
    "    def forward(self, pred, target, mask):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            pred: [B, 3, H, W] - Predicted (inpainted) image\n",
    "            target: [B, 3, H, W] - Ground truth clean image\n",
    "            mask: [B, 1, H, W] - Binary mask (1=valid, 0=hole)\n",
    "        \"\"\"\n",
    "        # Expand mask\n",
    "        mask_3ch = mask.repeat(1, 3, 1, 1)\n",
    "        \n",
    "        # Valid loss (on originally valid pixels)\n",
    "        valid_loss = self.l1_loss(pred * mask_3ch, target * mask_3ch)\n",
    "        \n",
    "        # Hole loss (on filled regions)\n",
    "        hole_loss = self.l1_loss(pred * (1 - mask_3ch), target * (1 - mask_3ch))\n",
    "        \n",
    "        # Perceptual loss\n",
    "        perceptual_loss = self.perceptual_loss(pred, target)\n",
    "        \n",
    "        # Style loss\n",
    "        style_loss = self.style_loss(pred, target)\n",
    "        \n",
    "        # Total variation loss (smoothness)\n",
    "        tv_loss = self.tv_loss(pred * (1 - mask_3ch))\n",
    "        \n",
    "        # Weighted combination\n",
    "        total_loss = (\n",
    "            1.0 * valid_loss +\n",
    "            6.0 * hole_loss +\n",
    "            0.05 * perceptual_loss +\n",
    "            120.0 * style_loss +\n",
    "            0.1 * tv_loss\n",
    "        )\n",
    "        \n",
    "        return total_loss, {\n",
    "            'valid': valid_loss.item(),\n",
    "            'hole': hole_loss.item(),\n",
    "            'perceptual': perceptual_loss.item(),\n",
    "            'style': style_loss.item(),\n",
    "            'tv': tv_loss.item()\n",
    "        }\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 4. DATASET FOR TRAINING\n",
    "# =============================================================================\n",
    "\n",
    "class InpaintingDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for training inpainting model\n",
    "    Uses your existing U-Net to generate masks OR synthetic masks\n",
    "    \"\"\"\n",
    "    def __init__(self, image_dir, image_files, mask_predictor=None, \n",
    "                 img_size=256, use_synthetic_masks=True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            image_dir: Directory with images\n",
    "            image_files: List of image filenames\n",
    "            mask_predictor: Your trained U-Net (optional)\n",
    "            img_size: Image size\n",
    "            use_synthetic_masks: If True, generate random masks\n",
    "        \"\"\"\n",
    "        self.image_dir = image_dir\n",
    "        self.image_files = image_files\n",
    "        self.mask_predictor = mask_predictor\n",
    "        self.img_size = img_size\n",
    "        self.use_synthetic_masks = use_synthetic_masks\n",
    "    \n",
    "    def generate_random_mask(self, img_size):\n",
    "        \"\"\"Generate random irregular mask\"\"\"\n",
    "        mask = np.ones((img_size, img_size), dtype=np.float32)\n",
    "        \n",
    "        # Random rectangles\n",
    "        num_rects = np.random.randint(1, 5)\n",
    "        for _ in range(num_rects):\n",
    "            x1 = np.random.randint(0, img_size - 20)\n",
    "            y1 = np.random.randint(0, img_size - 20)\n",
    "            w = np.random.randint(20, img_size // 3)\n",
    "            h = np.random.randint(20, img_size // 3)\n",
    "            mask[y1:y1+h, x1:x1+w] = 0\n",
    "        \n",
    "        # Random lines (scratches)\n",
    "        num_lines = np.random.randint(0, 10)\n",
    "        for _ in range(num_lines):\n",
    "            x1, y1 = np.random.randint(0, img_size, 2)\n",
    "            x2, y2 = np.random.randint(0, img_size, 2)\n",
    "            thickness = np.random.randint(1, 5)\n",
    "            cv2.line(mask, (x1, y1), (x2, y2), 0, thickness)\n",
    "        \n",
    "        # Dilate mask slightly\n",
    "        kernel = np.ones((3, 3), np.uint8)\n",
    "        mask = cv2.erode(mask, kernel, iterations=1)\n",
    "        \n",
    "        return mask\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Load image\n",
    "        img_path = os.path.join(self.image_dir, self.image_files[idx])\n",
    "        image = cv2.imread(img_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        image = cv2.resize(image, (self.img_size, self.img_size))\n",
    "        \n",
    "        # Generate or predict mask\n",
    "        if self.use_synthetic_masks or self.mask_predictor is None:\n",
    "            mask = self.generate_random_mask(self.img_size)\n",
    "        else:\n",
    "            # Use your trained U-Net to predict mask\n",
    "            with torch.no_grad():\n",
    "                img_tensor = torch.from_numpy(image).permute(2, 0, 1).float() / 255.0\n",
    "                img_tensor = img_tensor.unsqueeze(0)\n",
    "                mask = self.mask_predictor.predict(image, threshold=0.5)\n",
    "        \n",
    "        # Convert to tensors\n",
    "        image = torch.from_numpy(image).permute(2, 0, 1).float() / 127.5 - 1.0  # [-1, 1]\n",
    "        mask = torch.from_numpy(mask).unsqueeze(0).float()\n",
    "        \n",
    "        # Masked image (fill holes with mean)\n",
    "        masked_image = image * mask\n",
    "        \n",
    "        return masked_image, mask, image  # masked, mask, ground_truth\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 5. TRAINING FUNCTION\n",
    "# =============================================================================\n",
    "\n",
    "def train_inpainting_model(\n",
    "    image_dir,\n",
    "    train_files,\n",
    "    val_files,\n",
    "    epochs=100,\n",
    "    batch_size=8,\n",
    "    learning_rate=0.0002,\n",
    "    img_size=256,\n",
    "    save_dir='/kaggle/working/inpainting_checkpoints',\n",
    "    device='cuda',\n",
    "    mask_predictor=None,\n",
    "    use_synthetic_masks=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Train partial convolution inpainting model\n",
    "    \"\"\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(\"PARTIAL CONVOLUTION INPAINTING TRAINING\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Device: {device}\")\n",
    "    print(f\"Training images: {len(train_files)}\")\n",
    "    print(f\"Validation images: {len(val_files)}\")\n",
    "    print(f\"Epochs: {epochs}\")\n",
    "    print(f\"Batch size: {batch_size}\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = InpaintingDataset(\n",
    "        image_dir, train_files, mask_predictor, \n",
    "        img_size, use_synthetic_masks\n",
    "    )\n",
    "    val_dataset = InpaintingDataset(\n",
    "        image_dir, val_files, mask_predictor,\n",
    "        img_size, use_synthetic_masks\n",
    "    )\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, batch_size=batch_size,\n",
    "        shuffle=True, num_workers=2, pin_memory=True\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset, batch_size=batch_size,\n",
    "        shuffle=False, num_workers=2, pin_memory=True\n",
    "    )\n",
    "    \n",
    "    # Create model\n",
    "    model = PartialConvUNet().to(device)\n",
    "    \n",
    "    # Count parameters\n",
    "    total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"Total trainable parameters: {total_params:,}\")\n",
    "    \n",
    "    # Loss and optimizer\n",
    "    criterion = InpaintingLoss(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.5)\n",
    "    \n",
    "    # Training loop\n",
    "    best_val_loss = float('inf')\n",
    "    history = {'train_loss': [], 'val_loss': []}\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        print(f\"\\nEpoch {epoch+1}/{epochs}\")\n",
    "        print(\"-\"*70)\n",
    "        \n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_losses_detail = {'valid': 0, 'hole': 0, 'perceptual': 0, 'style': 0, 'tv': 0}\n",
    "        \n",
    "        pbar = tqdm(train_loader, desc='Training')\n",
    "        for masked_img, mask, gt_img in pbar:\n",
    "            masked_img = masked_img.to(device)\n",
    "            mask = mask.to(device)\n",
    "            gt_img = gt_img.to(device)\n",
    "            \n",
    "            # Forward\n",
    "            optimizer.zero_grad()\n",
    "            pred_img = model(masked_img, mask)\n",
    "            \n",
    "            # Loss\n",
    "            loss, loss_dict = criterion(pred_img, gt_img, mask)\n",
    "            \n",
    "            # Backward\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Accumulate losses\n",
    "            train_loss += loss.item()\n",
    "            for key in loss_dict:\n",
    "                train_losses_detail[key] += loss_dict[key]\n",
    "            \n",
    "            pbar.set_postfix({'loss': loss.item()})\n",
    "        \n",
    "        train_loss /= len(train_loader)\n",
    "        for key in train_losses_detail:\n",
    "            train_losses_detail[key] /= len(train_loader)\n",
    "        \n",
    "        history['train_loss'].append(train_loss)\n",
    "        \n",
    "        print(f\"Train Loss: {train_loss:.4f}\")\n",
    "        print(f\"  Valid: {train_losses_detail['valid']:.4f}, \"\n",
    "              f\"Hole: {train_losses_detail['hole']:.4f}, \"\n",
    "              f\"Perceptual: {train_losses_detail['perceptual']:.4f}\")\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for masked_img, mask, gt_img in tqdm(val_loader, desc='Validation'):\n",
    "                masked_img = masked_img.to(device)\n",
    "                mask = mask.to(device)\n",
    "                gt_img = gt_img.to(device)\n",
    "                \n",
    "                pred_img = model(masked_img, mask)\n",
    "                loss, _ = criterion(pred_img, gt_img, mask)\n",
    "                val_loss += loss.item()\n",
    "        \n",
    "        val_loss /= len(val_loader)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        \n",
    "        print(f\"Val Loss: {val_loss:.4f}\")\n",
    "        \n",
    "        # Learning rate scheduling\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Save best model\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'val_loss': val_loss,\n",
    "                'train_loss': train_loss\n",
    "            }, os.path.join(save_dir, 'best_inpainting_model.pth'))\n",
    "            print(f\"✓ Saved best model (val_loss: {val_loss:.4f})\")\n",
    "        \n",
    "        # Save checkpoint every 10 epochs\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'val_loss': val_loss\n",
    "            }, os.path.join(save_dir, f'checkpoint_epoch_{epoch+1}.pth'))\n",
    "        \n",
    "        # Visualize samples every 10 epochs\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            visualize_results(model, val_loader, device, \n",
    "                            save_path=os.path.join(save_dir, f'samples_epoch_{epoch+1}.png'))\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"Training completed!\")\n",
    "    print(f\"Best validation loss: {best_val_loss:.4f}\")\n",
    "    \n",
    "    # Save history\n",
    "    import pickle\n",
    "    with open(os.path.join(save_dir, 'training_history.pkl'), 'wb') as f:\n",
    "        pickle.dump(history, f)\n",
    "    \n",
    "    return model, history\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 6. VISUALIZATION\n",
    "# =============================================================================\n",
    "\n",
    "def visualize_results(model, dataloader, device, save_path=None, num_samples=4):\n",
    "    \"\"\"Visualize inpainting results\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Get samples\n",
    "    masked_imgs, masks, gt_imgs = next(iter(dataloader))\n",
    "    masked_imgs = masked_imgs[:num_samples].to(device)\n",
    "    masks = masks[:num_samples].to(device)\n",
    "    gt_imgs = gt_imgs[:num_samples].to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        pred_imgs = model(masked_imgs, masks)\n",
    "    \n",
    "    # Convert to numpy\n",
    "    def tensor_to_img(tensor):\n",
    "        img = tensor.cpu().numpy()\n",
    "        img = (img + 1) / 2.0  # [-1, 1] -> [0, 1]\n",
    "        img = np.transpose(img, (0, 2, 3, 1))\n",
    "        return np.clip(img, 0, 1)\n",
    "    \n",
    "    masked_np = tensor_to_img(masked_imgs)\n",
    "    pred_np = tensor_to_img(pred_imgs)\n",
    "    gt_np = tensor_to_img(gt_imgs)\n",
    "    masks_np = masks.cpu().numpy().transpose(0, 2, 3, 1)\n",
    "    \n",
    "    # Plot\n",
    "    fig, axes = plt.subplots(num_samples, 4, figsize=(16, 4*num_samples))\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        axes[i, 0].imshow(masked_np[i])\n",
    "        axes[i, 0].set_title('Masked Input')\n",
    "        axes[i, 0].axis('off')\n",
    "        \n",
    "        axes[i, 1].imshow(masks_np[i], cmap='gray')\n",
    "        axes[i, 1].set_title('Mask')\n",
    "        axes[i, 1].axis('off')\n",
    "        \n",
    "        axes[i, 2].imshow(pred_np[i])\n",
    "        axes[i, 2].set_title('Predicted')\n",
    "        axes[i, 2].axis('off')\n",
    "        \n",
    "        axes[i, 3].imshow(gt_np[i])\n",
    "        axes[i, 3].set_title('Ground Truth')\n",
    "        axes[i, 3].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 7. INFERENCE CLASS\n",
    "# =============================================================================\n",
    "\n",
    "class InpaintingPredictor:\n",
    "    \"\"\"Easy-to-use wrapper for inference\"\"\"\n",
    "    \n",
    "    def __init__(self, checkpoint_path, device='cuda'):\n",
    "        self.device = device\n",
    "        self.model = PartialConvUNet().to(device)\n",
    "        \n",
    "        checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "        self.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        self.model.eval()\n",
    "        \n",
    "        print(f\"✓ Inpainting model loaded from {checkpoint_path}\")\n",
    "        print(f\"  Epoch: {checkpoint['epoch']}\")\n",
    "        print(f\"  Val Loss: {checkpoint['val_loss']:.4f}\")\n",
    "    \n",
    "    def inpaint(self, image, mask):\n",
    "        \"\"\"\n",
    "        Inpaint image\n",
    "        \n",
    "        Args:\n",
    "            image: numpy array [H, W, 3], RGB, [0-255]\n",
    "            mask: numpy array [H, W], binary [0-1]\n",
    "        \n",
    "        Returns:\n",
    "            inpainted: numpy array [H, W, 3], RGB, [0-255]\n",
    "        \"\"\"\n",
    "        original_size = image.shape[:2]\n",
    "        \n",
    "        # Preprocess\n",
    "        image = cv2.resize(image, (256, 256))\n",
    "        mask = cv2.resize(mask, (256, 256))\n",
    "        \n",
    "        # To tensor\n",
    "        image_tensor = torch.from_numpy(image).permute(2, 0, 1).float() / 127.5 - 1.0\n",
    "        mask_tensor = torch.from_numpy(mask).unsqueeze(0).float()\n",
    "        \n",
    "        # Masked input\n",
    "        masked_input = image_tensor * mask_tensor\n",
    "        \n",
    "        # Add batch dimension\n",
    "        masked_input = masked_input.unsqueeze(0).to(self.device)\n",
    "        mask_tensor = mask_tensor.unsqueeze(0).to(self.device)\n",
    "        \n",
    "        # Inpaint\n",
    "        with torch.no_grad():\n",
    "            pred = self.model(masked_input, mask_tensor)\n",
    "        \n",
    "        # Postprocess\n",
    "        pred = pred.squeeze().cpu().numpy()\n",
    "        pred = (pred + 1) / 2.0 * 255.0\n",
    "        pred = np.transpose(pred, (1, 2, 0))\n",
    "        pred = np.clip(pred, 0, 255).astype(np.uint8)\n",
    "        \n",
    "        # Resize back\n",
    "        if pred.shape[:2] != original_size:\n",
    "            pred = cv2.resize(pred, (original_size[1], original_size[0]))\n",
    "        \n",
    "        return pred\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 8. KAGGLE USAGE EXAMPLE\n",
    "# =============================================================================\n",
    "\n",
    "def kaggle_train_inpainting():\n",
    "    \"\"\"\n",
    "    Complete training pipeline for Kaggle\n",
    "    \"\"\"\n",
    "    \n",
    "    # Paths\n",
    "    image_dir = '/kaggle/input/flickr8k/Images'  # Adjust path\n",
    "    save_dir = '/kaggle/working/inpainting_checkpoints'\n",
    "    \n",
    "    # Load data split (from your U-Net training)\n",
    "    import pickle\n",
    "    with open('/kaggle/input/mask-predictor-model-u-net-style/checkpoints/data_split.pkl', 'rb') as f:\n",
    "        split_info = pickle.load(f)\n",
    "    \n",
    "    train_files = split_info['train']\n",
    "    val_files = split_info['val']\n",
    "    \n",
    "    # Load your trained U-Net (optional - can use synthetic masks)\n",
    "    # from mask_predictor import MaskPredictor\n",
    "    # mask_predictor = MaskPredictor('/kaggle/input/your-unet-dataset/best_model.pth')\n",
    "    mask_predictor = None  # Use synthetic masks for simplicity\n",
    "    \n",
    "    # Train\n",
    "    model, history = train_inpainting_model(\n",
    "        image_dir=image_dir,\n",
    "        train_files=train_files,\n",
    "        val_files=val_files,\n",
    "        epochs=50,  # Start with 50, increase if needed\n",
    "        batch_size=8,\n",
    "        learning_rate=0.0002,\n",
    "        img_size=256,\n",
    "        save_dir=save_dir,\n",
    "        device='cuda' if torch.cuda.is_available() else 'cpu',\n",
    "        mask_predictor=mask_predictor,\n",
    "        use_synthetic_masks=True\n",
    "    )\n",
    "    \n",
    "    print(\"\\n✓ Training complete!\")\n",
    "    print(f\"Best model saved at: {save_dir}/best_inpainting_model.pth\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 9. COMPLETE PIPELINE: U-NET + INPAINTING\n",
    "# =============================================================================\n",
    "\n",
    "def complete_pipeline_demo():\n",
    "    \"\"\"\n",
    "    Demo: Use U-Net to predict mask, then inpaint\n",
    "    \"\"\"\n",
    "    # Load models\n",
    "    # from mask_predictor import MaskPredictor\n",
    "    mask_predictor = MaskPredictor('/kaggle/input/u-net-model-trained/best_model.pth')\n",
    "    inpainting_model = InpaintingPredictor('/kaggle/input/partial-convolution-model-trained/best_inpainting_model.pth')\n",
    "    \n",
    "    # Load test image\n",
    "    image = cv2.imread('/kaggle/input/flickr8k/Images/3226254560_2f8ac147ea.jpg')\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Step 1: Predict mask\n",
    "    mask = mask_predictor.predict(image, threshold=0.5)\n",
    "    \n",
    "    # Step 2: Inpaint\n",
    "    inpainted = inpainting_model.inpaint(image, mask)\n",
    "    \n",
    "    # Visualize\n",
    "    fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
    "    axes[0].imshow(image)\n",
    "    axes[0].set_title('Original (Damaged)')\n",
    "    axes[1].imshow(mask, cmap='gray')\n",
    "    axes[1].set_title('Predicted Mask')\n",
    "    axes[2].imshow(inpainted)\n",
    "    axes[2].set_title('Inpainted')\n",
    "    \n",
    "    # Show difference\n",
    "    diff = np.abs(image.astype(float) - inpainted.astype(float)).mean(axis=2)\n",
    "    axes[3].imshow(diff, cmap='hot')\n",
    "    axes[3].set_title('Difference')\n",
    "    \n",
    "    for ax in axes:\n",
    "        ax.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('/kaggle/working/complete_pipeline_result.png', dpi=150)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Run training\n",
    "    # kaggle_train_inpainting()\n",
    "    complete_pipeline_demo()\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Inference Pipeline: We can infer a batch of images from a directory and can get the inferred results in an output directory.\n",
    "# Input: input_directory_path\n",
    "# Output: output_directory_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-12T00:24:53.262451Z",
     "iopub.status.busy": "2025-11-12T00:24:53.262241Z",
     "iopub.status.idle": "2025-11-12T00:24:56.746993Z",
     "shell.execute_reply": "2025-11-12T00:24:56.746010Z",
     "shell.execute_reply.started": "2025-11-12T00:24:53.262435Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Find images (adjust path if needed)\n",
    "image_dir = '/kaggle/input/flickr8k/Images'  # or your image path\n",
    "output_dir = '/kaggle/working/outputs'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Get 20 random images\n",
    "files = [f for f in os.listdir(image_dir) if f.endswith(('.jpg', '.png'))]\n",
    "selected = random.sample(files, 20)\n",
    "\n",
    "# Load models\n",
    "# from mask_predictor import MaskPredictor, SyntheticDistortionGenerator\n",
    "# from inpainting import InpaintingPredictor\n",
    "\n",
    "mask_model = MaskPredictor('/kaggle/input/u-net-model-trained/best_model.pth')\n",
    "inpaint_model = InpaintingPredictor('/kaggle/input/partial-convolution-model-trained/best_inpainting_model.pth')\n",
    "distortion_gen = SyntheticDistortionGenerator()\n",
    "\n",
    "# Process\n",
    "for i, fname in enumerate(selected, 1):\n",
    "    img = cv2.cvtColor(cv2.imread(f'{image_dir}/{fname}'), cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    distorted, mask_gt = distortion_gen.generate_distortion(img)\n",
    "    mask = mask_model.predict(distorted)\n",
    "    result = inpaint_model.inpaint(distorted, mask)\n",
    "    \n",
    "    cv2.imwrite(f'{output_dir}/test_image_{i}.png', cv2.cvtColor(img, cv2.COLOR_RGB2BGR))\n",
    "    cv2.imwrite(f'{output_dir}/test_image_{i}_distorted.png', cv2.cvtColor(distorted, cv2.COLOR_RGB2BGR))\n",
    "    mask_vis = cv2.cvtColor((mask * 255).astype(np.uint8), cv2.COLOR_GRAY2BGR)\n",
    "    cv2.imwrite(f'{output_dir}/test_image_{i}_pred_mask.png', mask_vis)\n",
    "    cv2.imwrite(f'{output_dir}/test_image_{i}_restored.png', cv2.cvtColor(result, cv2.COLOR_RGB2BGR))\n",
    "\n",
    "print(f\"✓ Saved to {output_dir}\")\n",
    "\n",
    "import shutil\n",
    "zip_path = '/kaggle/working/inpainting_outputs'\n",
    "shutil.make_archive(zip_path, 'zip', output_dir)\n",
    "print(f\"✓ Zipped successfully: {zip_path}.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 623289,
     "sourceId": 1111676,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8694520,
     "sourceId": 13673844,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8694600,
     "sourceId": 13673948,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
